\documentclass[11pt, hidelinks]{report}
\usepackage[margin=0.5in]{geometry}
\usepackage{indentfirst}
\usepackage{hyphenat}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{tabto}
\usepackage{xfrac}
\usepackage[bottom]{footmisc}
\usepackage{xparse}
\usepackage{xcolor}
\usepackage{listings}


\makeatletter
\renewcommand{\thesection}{%
  \ifnum\c@chapter<1 \@arabic\c@section
  \else \thechapter.\@arabic\c@section
  \fi
}
\makeatother

\newcommand{\refapp}[1]{\textsuperscript{[\ref{#1}]}}

\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}

\title{An Overview of the RobERT (Robotic Environmental Rating Tool) Platform for the Purpose of Autonomous Agricultural Data Acquisition and Actuation in a Miniaturized Mock Cornfield Environment \\ \bigskip \bigskip
\Large 2020 ASABE Student Robotics Challenge: Standard Group}
\author{Hunter Halloran, William Snapp, Kevin Koffroth, Michael Starks, \\Caroline Lassiter, Cole Sterck, Jake Alford, Tommy Anton, John Bare\thanks {Mentored by Changying ``Charlie" Li, Ph.D. and Jawad Iqbal}\textsuperscript{,}\thanks{Sponsored by the University of Georgia College of Engineering, Fred Beyette Jr., Ph.D., the Bio-Sensing and Instrumentation Lab, AGCO, and IGUS} \\ \\ The University of Georgia}
\date{5 July, 2020}


\begin{document}
\begin{titlepage}
\maketitle
\end{titlepage}

\addtocontents{toc}{\vspace{-4ex}}
\newpage
\setcounter{tocdepth}{3}
\raggedbottom
\tableofcontents
\newpage

\section{Introduction}
\subsection{Need In Agriculture and Approach}
While last year this team faced the issue of designing the robot only to meet the task of the competition and failing to consider a net benefit to agricultural robotics, this robot was designed targeting real world surrogates to represent real needs in autonomous field robotics. This system was designed as a modified shadow of the Bio-Sensing and Instrumentation Lab’s MARS robot, a system designed to drive over crop rows to image on all sides with a 4-wheel steering mechanism for absolute positioning control. In the shrunken scale, the competition robot was able to benefit from design considerations already realized physically from the MARS platform while being able to more rapidly consider other methods in the miniature construction, allowing for the possibility of multi-row processing in parallel without needing to build a full robot in full crop-field size. 

This platform demonstrates a positive impact for autonomous field platforms due to the aforementioned considerations. By using a large form factor capable of driving over crop rows, the system can have more opportunities for data acquisition in various positions, by either taking multiple images and other data readings simultaneously or being able to readjust a single camera until a suitable position for data accumulation has been found. Further, by having an extensible platform created with the intention of possible modification due to modularity as an internal design requirement, other systems can also be added, such as actuation platforms demonstrated in this robot as the seed dispensers. In this case, the modular nature comes from replaceable acrylic panels capable of being removed and replaced with a new panel with required mount points, but as this system exists as a concept to be scaled as the MARS system, aluminum extrusion is the conceptual equivalent for new mount points.

\subsection{Definition of Design Objectives}
The overall robotics system implemented multiple internal requirements beyond the necessary scope of the competition for the purposes of creating a generalized system to demonstrate the abilities of a platform rather than designing only for the 2020 ASABE Student Robotics Challenge. For this reason, molecularity and efficiency were large goals for the design. It was chosen to create an expanding assembly to be able to compromise between a desire to accumulate data for the play-field in a parallelized manner while meeting the competition requirements of beginning within a one square cubic foot volume. Due to a desire to mirror other agricultural robotics systems from the University of Georgia as a miniaturized testbed, the design needed to include a 4-wheel steering design with four individually steerable wheels to mimic the MARS platform and Mini-Flowerbot designs from B-SAIL\footnote{The Bio-Sensing and Instrumentation Lab (\url{https://sensinglab.engr.uga.edu/})}, as well as being able to demonstrate a more realistic means of navigation over a crop field rather than omnidirectional wheels which would not be able to function in a soil environment. 

The design of the overall system also needed to be extensible to be able to include a suite of sensors and actuators to achieve the required task for the challenge. In this case, the requirement is the mounting of cameras for both plant health detection and line following, as well as actuators for the dispensing of seeds for missing crops. An internal requirement was created of designing using primarily acrylic sheets to allow for rapid prototyping with locked 2D pieces rather than a slower process of 3D printing each necessary component. For the purposes of distinguishing between stressed and unstressed plants a visual identification system is required using cameras and image processing. It was pushed to make use of a deep learning application to complete this to align more closely with actual field robotics.

\section{Hardware Description}
Graphics and supplementary information are available in the appendix\refapp{app:mech}. 
\subsection{Overview}
For the mechanical design of the robot, the team wanted to optimize for speed, accuracy, and weight of the robot. The design process began prior to the release of the rules with the design of a simple test platform to prototype the line following and navigation systems. The test platform used a square of aluminum extrusion with stepper motors for actuation and Mecanum\footnote{Mecanum wheels are made up of slanted rollers placed along the edge of the wheel, allowing for forward and backward movement, as well as sideways movement by engaging the motors in opposite directions. (\url{https://en.wikipedia.org/wiki/Mecanum_wheel})} wheels to move forwards and backwards as well as strafe laterally.  This design was abandoned as soon as rules were released due to the weight of the motors as well as the aluminum extrusion.  As a next step, the team decided to use a body made of laser-cut acrylic moving forward, due to the customizability, ease of use, and weight reduction provided by a lighter and thinner material.

The first competition-specific design built was a panel-based design, with cameras and droppers mounted on each side.  The cameras were mounted on panels that deployed out sideways, blocking out ambient light and shining LED lights to provide optimal lighting for image processing. This design was decided on due to its ability to scan 2 rows at once, allowing the system to only make 2 passes through the course, instead of 4.  

The mechanical design process was split into 2 phases, with a Phase I\refapp{app:phaseone} that was mainly for gauging the capabilities of both our hardware and software, and Phase II\refapp{app:phasetwo} based around building an expanding design that pushed our capabilities and maximized our points and efficiency. Separate chassis designs were essential to developing different mechanical subsystems.

\subsection{Mechanical Seed Dropper}
A central component to every iteration of the robot has been the ability to dispense the simulated seeds in a purely mechanical way. The team believed a purely mechanical design to be the most efficient method because of the perceived software overhead required to classify the position as a germination position, and then use some form of actuation to dispense the seed. We accomplished this with a ``seed dropper arm" assembly that extends out from the side of the robot to interface with each plant position. The dropper arm consists of two distinct parts: an upper portion holding the simulated seeds and connecting the dropper assembly to the robot, and a lower assembly which is spring-loaded at the joint with the robot and at the joint with the lower assembly, where the spring is engaged when it collides with the cups defined as a non-germination position. It pulls the lowest seed from the column through an opening in the holder and drops it into the seed cup, before being stopped by the mechanism’s end piece and engaging the stronger inner spring that pushes the dropper past the cup.  Should it collide with an already grown corn stalk, the upper portion will make contact first, and the dropper will simply slide off without engaging the lower portion. The first dropper iteration was far too heavy, and used springs to actuate the dropping mechanism.  This system was unwieldy, causing strain on the chassis of the robot, so we redesigned the dropper.  We narrowed the column that holds the seeds and reduced the body significantly, while optimizing 3D print settings in order to lighten it further.  The bottom portion was narrowed considerably and thinned out, and the pivot between the upper and lower was redesigned to use a smaller bolt, reducing the weight by narrowing the bolt from a \sfrac{1}{4} inch bolt to an M2 bolt.  The spring was also changed to a rubber band, increasing consistency and further reducing weight.  The distance required to actuate the mechanism was also reduced.

The droppers had to be fixed to make them slightly longer and attached to a ROBOTIS Dynamixel\footnote{Dynamixel motors are a brand of particularly capable smart actuators. (\url{https://www.robotis.us/dynamixel/})} motor.  The motors are used to measure resistance, and contact with a plant will cause the cameras to take a picture to determine the state of the plant.  
The last major change made to the chassis was the replacement of the drawer slides.  The first drawer slides we chose did not extend smoothly or fully extend without extra force.  We changed them to a different drawer slide. The sides also had a tendency to recompress during the course of each run, so some 3D printed locks were made to lock open the side sections once they expanded.

The dropper length had to be further adjusted after this due to the minor extension difference in the drawer slides.  This was to enable them to more accurately hit each empty spot. To further increase the effectiveness of each dropper, the bottom portion was changed from a straight piece pressing against each cup to a cupped section that wraps around each seed-holding cup.  This would allow the position of each seed to accurately drop directly into the center of each cup, preventing them from falling out or getting jammed.  We also adjusted the inside dropper connecting section to curve away from the body sooner, allowing them to avoid the camera mount on the bottom of the robot.

\subsection{Drive System}
The drive system we used was based around Mecanum wheels at first. These wheels were satisfactory for making a test platform, but after some testing we realized that they introduced too much slip.  As mentioned prior, the primary aspect of the MARS robot that was mirrored in RobERT is the {360\textdegree} steering system. In this steering system there are two motors attached to each wheel: one to control angular rotation and the other to control transverse movement.  The motor that controls wheel rotation is mounted such that the axis of rotation passes through the center of the point of contact for the wheel, meaning that they rotate exactly on axis, preventing slip while allowing the wheels to rotate the body or strafe in any direction at any time.  This system makes navigating the course much easier and provides more options for movement when compared to Mecanum wheels.

The first iteration of the {360\textdegree} steering had bulkier motor mounts than necessary and used wheels that didn't have enough grip to accurately navigate the course. To fix this, we slimmed down the motor mounts and thinned the walls, and purchased new wheels with smooth rubber designed to eliminate slip.  

\subsection{Chassis}
After the design of the first phase of our chassis, we decided that in order to increase the speed of each run, we should move to an expanding design that could scan all four rows at once.  To accomplish this, our chassis is constructed of three main portions.  An inner portion houses the main electronic components, holds the droppers and cameras for the two inside rows.  This section is held up by two outer sections that hold the driving mechanism and the outside droppers and cameras.  The driving mechanism is used to expand the robot without the need for extra actuation.  The robot begins in the corner, and the two outer sections drive to their expanded location, while holding up the central section with drawer slides.  The closest outer section navigates to between the first and second row, and the other outer section navigates to between the third and fourth row, while suspending the center section over the area between the second and third rows.  The cameras are not able to be rigidly mounted in place due to the size constraints of the robot.  Instead of using panels, we decided to have the cameras mounted on mounts that deploy via smaller servos at a {45\textdegree} angle to the angle of the ground.  

\subsection{Computational Hardware}
The team made careful considerations on how to spend the computational points for the purposes of this challenge. To compete for the Standard Group competition, it was a requirement to spend 6 points or less to avoid penalty. Knowing that we would need at least one microcontroller of an Arduino or similar, we allocated one point to those and gave a 5 point limit for the main computer of the robot. Having used Raspberry Pis prior, they seemed a safe option, counting exactly 5 points for a 4th generation device of the 1 gigabyte of RAM version. More effort was made beyond this to try to find a device which would have more power for the same point value, especially in vision applications. Research led to the Coral Dev Board\footnote{The Dev Board is a single-board computer that's ideal when you need to perform fast machine learning (ML) inferencing in a small form factor. (\url{https://coral.ai/products/dev-board/})}, a device designed for machine learning, which was the process the team intended on using for the plant health classification task. Having 4 primary processing cores and 1 gigabyte of RAM, it met the internal team requirements for decision as well as having an integrated Edge TPU capable of quickly processing neural networks for the cornstalk health determination. Rather than ultimately using an Arduino as a microcontroller, an ArbotiX-M Robocontroller\footnote{The ArbotiX-M Robocontroller is an Arduino compatible board designed specifically for use with Dynamixel and standard servo motors for a more integrated control environment. (\url{https://www.trossenrobotics.com/p/arbotix-robot-controller.aspx})} was chosen as it provides specific headers for both standard servos as well as Dynamixel motors, allowing for easier power delivery and data transmission to the devices. The final circuit diagram of the system can be viewed in the appendix\refapp{app:circuit}.

\section{Software Overview}
Graphics and supplementary information are available in the appendix\refapp{app:nav}\refapp{app:vision}\refapp{app:system}. Source code for the project is also available\refapp{app:github}.
\subsection{Navigation and Drive System}
Using OpenCV\footnote{OpenCV is an open source computer vision library. (\url{https://opencv.org/})} the camera identifies contours\footnote{Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. (\url{https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html})}. This process entails converting the frame to gray-scale so that different shades detected within the frame, and whose colors fit within the chosen shade range (black/dark grey), can be identified. This range is determined by passing in a range of RGB values. Based on the list of contours found in the frame, the robot finds the largest of these contours and uses them to identify the black tape. To account for any light reflecting off of the tape, the Robot considers a range of greys rather than solely black to detect the maximum number of points in the frame.

The angle is identified by determining the center x-coordinate of the frame making the center of the frame the robot’s focal point. The robot then determines the top middle coordinate, within the contour, by observing the x-values of coordinates with lower y-values\refapp{app:ogline} in the contour closest to the front portion of the tape. Then, the difference between the robot’s focal point and the tape’s top-center coordinate is found, storing the distance as x and y values. Using trigonometry, the angle can be calculated with the gathered values, converted to degrees. 
The translation of the robot, off of the tape, is identified by finding a rectangle that approximates the shape of the tape. The contours of the approximation are filtered to those concentric with the focal point of the camera, we compare it to the center point of the frame, calculating the horizontal distance between the robot’s focal point and the center of the tape, allowing us to determine the distance that the robot needs to travel, to realign its focal point with the center of the tape. Both values are returned form \codeword{line_tracing} to determine correction.

The robot navigates the field guided by a path defined by black electrical tape. The position of the robot is a combination of its angular and linear positions. Using a polar coordinate system allows corrections to be completed in two distinct operations: angle corrections to fix heading and linear correction to fix drift laterally from the desired path. The robot aims to maintain a heading \sfrac{+}{-} {0.3\textdegree} from zero and \sfrac{+}{-} 0.1 inches from the center line. To reduce the impact that path correction has on completing the task, angle correction is completed before lateral corrections.

The robot initially used Ackermann steering to complete course correction in one operation. Ackermann steering combines the angular and lateral correction while maintaining forward. The inside and outside wheel angles are calculated through the following formulas where $\theta_i$ represents the inner wheel angle, $\theta_o$ is the outside wheel angle, $\theta$ is the desired turn angle, $l$ is the length of the robot, and $w$ is the robot width:
\begin{equation}
    \theta_i=\tan^{-1}\left(\frac{2l \times\sin\theta}{2l \times\cos\theta-w\sin\theta}\right)
\end{equation}
\begin{equation}
    \theta_o=\tan^{-1}\left(\frac{2l \times\sin\theta}{2l \times\cos\theta+w\sin\theta}\right)
\end{equation}
Due to physical limitations of the robot, correction was divided into two separate steps angular and lateral correction. 

Angular correction is done using center axis turning. The amount of time needed to turn through an angle is calculated using the following formula where $T$ is the time the motor should spin for and $\theta$ is the desired angle of rotation about the center axis:
\begin{equation}
    T=\frac{\theta}{\pi}\times44.75
\end{equation}
Center axis turning has all of the robot’s wheels turn normal to the radius of the robot moving in the same direction. The current angle of the robot is retrieved from the line tracking code and the robot moves in the opposite direction of the current angular offset to correct the heading to be parallel to the tape path. The robot rotates in place, realigning with the crop rows before calculating the lateral correction path. To preform this correction, the following formulas are used where $\Delta$ represents the error distance, $D$ represents the distance to travel to correct, and $\theta$ represents the calculated angle by which to correct the robot:
\begin{equation}
    D=\sqrt{\Delta^2+2^2}
\end{equation}
\begin{equation}
    \theta=\tan^{-1}\left(\frac{\Delta}{2}\right)
\end{equation}

Correcting the heading of the robot by turning on the center axis simplifies the lateral correction procedures. The lateral correction vector is calculated assuming the robot’s heading is parallel to the taped path, the distance from the path is retrieved from the line tracking code, and a correction path is calculated using the error distance and a two-inch run to maintain forward motion while correcting the path. Lateral correction uses \codeword{crab_steering} in the \codeword{Robot} object, moving the wheels to create a vector in the desired direction instead of moving, the entire robot corrects faster and minimizes alignment error with crops. 

\subsection{Image Capture and Pre-processing}
Classifying each germination position on the competition board, was broken into two sub-tasks: capturing and pre-processing image data for each position, and constructing a reliable method of detecting the germination status with said image data. Image capture and processing, used OpenCV allowing us to programmatically capture and edit images taken by the cameras on the robot. We crop the images to reduce misidentification and generally restrict the data our detection system process to only include the target germination position. For our capture system to work, we needed a method of reliably detecting when to take a picture, i.e. when our camera is lined up with the next germination position\refapp{app:uncrop}\refapp{app:crop}. We initially prototyped a system using simple limit switches and a coffee stirrer. This worked okay when testing off of the robot, but when the stalks came in rapid succession, the limit switch never had enough room to release, causing several missed triggers\refapp{app:ogtrig}. The next iteration of the trigger system utilized the existing mechanical dropper design\refapp{app:Dropper}, along with a Dynamixel motor to create a spring-like trigger system that would capture an image after a set force is applied to the mechanical dropper. We did this by utilizing some of the smart actuator\footnote{A smart actuator is an actuator with a microcontroller, sensor(s), and communication unit(s). It typically communicates with a host device to deliver various data such as position, load, sensor data, etc.} capabilities of the Dynamixel motors, one of which is a readable \codeword{Present_Load} value which represents the electric current through the motor at that time. Since the current sensing works by measuring any current through the motor, back-driving them also changes the \codeword{Present_Load} value, making it useful as a torque sensor. Using this functionality, we were able to model the Dynamixel as a spring, letting the stalk back drive the motor and trigger the camera, then when force is no longer being applied, the motor moves back to its original position to sense the next stalk\refapp{app:dyntrig}. The image capture and pre-processing code is divided into two classes: the \codeword{ImageCapture} class, which holds all of the OpenCV code for capturing and cropping images, and the \codeword{DynamixelTrigger} class, which holds the Dynamixel control code. These two classes communicate using Unix 'pipes', as describe in the Software Systems Integration section.

\subsection{Image Classification}
The detection software uses the post processed images from the cameras to classify each image by germination status. Initial efforts to accomplish this used a simple OpenCV script which would count the number of yellow, green, and white pixels in an image, and output that as a percentage\refapp{app:ogimg}. This worked well enough in simple cases, but occlusion\footnote{Occlude: to shut in, out, or off. (\url{https://www.dictionary.com/browse/occlude?s=t})} of the target crop made detection inaccurate. While we could have developed more advanced computer vision techniques using OpenCV, we decided to use a machine learning approach as we had an interest in learning it. 

Explorations into neural network toolkits lead us to the TensorFlow\footnote{TensorFlow is a software library frequently in machine learning applications. (\url{https://www.tensorflow.org/})} ecosystem, which is a free and open-source software library developed by Google and implemented frequently in machine learning applications. To assist us with the task of training our image classifier, we layered Keras\footnote{Keras is the high-level neural network API used in the TensorFlow ecosystem, it is included with TensorFlow when you install it on your system. (\url{https://www.infoworld.com/article/3336192/what-is-keras-the-deep-neural-network-api-explained.html})} and SciKit-learn\footnote{SciKit-learn is another high-level neural network API. While it can be used to create models, we are using it to manage the training of our neural network, collect and display metrics, and perform model selection. (\url{https://scikit-learn.org/stable/index.html})} on top of TensorFlow so that we could take advantage of their user-friendly APIs. Thanks to the open-source nature of these frameworks, it was easy for us to get started learning and experimenting using guides and tutorials as resources to achieve our goal of an accurate custom image classifier. Implementing our code using Anaconda\footnote{Anaconda is an easy to use Python workspace/environment manager. (\url{https://www.anaconda.com/})} and Jupyter Notebooks\footnote{Jupyter Notebooks is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. (\url{https://jupyter.org/})} allowed for quick prototyping and development, as Anaconda made managing Python environments a breeze, and Jupyter Notebooks allowed us to break our modeling/training code into smaller segments, an essential for debugging machine learning applications. 

The training for our model began by first collecting and organizing our training data. We did this by generating a random competition board using the \codeword{competition_gui} script, and taking 10 pictures of each position on the board, making sure to rotate the plants slightly between pictures. To more easily capture and organize the data, we developed a small OpenCV script which would capture, crop, and save images to predefined directories. This script was intentionally made to emulate how the photos would be captured and processed on the robot to eliminate variability in our model\refapp{app:util}. The images were sorted into yellow, green, and open directories to represent the three classes in our classification model. We used SK-learn’s LabelEncoder and One-Hot Encoding to organize the input and output variables into numeric form. Our data set was then split into a training set and validation set with a 1:4 ratio. As the names imply, the training set is used to train the neural net, while the validation set is used to test accuracy. A Keras Sequential model was then built out and image augmentation was implemented to prevent overfitting of the model. We then trained the model for 1500 epochs where we achieved a ~99\% validation accuracy\refapp{app:model}.

This gave us confidence in the ability of the model to correctly predict the plant status, but we were concerned with the speed of the neural net on the robot’s Single Board Computer. To run the classifier on the Google Coral Dev Board, we needed the model to be converted into a TensorFlow (TF) Lite model. Fortunately, the TF library includes a function to convert the Keras model to a TF Lite model. Running the neural net with TF Lite on the Coral Board resulted in classifications performed in half the time compared to a Raspberry Pi.

\subsection{Software System Integration}
Early on, we decided to program the robot in Python as it is a relatively easy language to program in, it would allow us to do rapid prototyping of our software systems. We had plans to migrate the code base over to C/C++ when most of the prototyping was finished, however due to time constraints and the inability to meet in person regularly, we had to abandon this endeavour and stick with Python. For performance reasons, and to more easily integrate all of our software systems, the team decided to utilize the Python multiprocessing module\footnote{The multiprocessing module provided by Python is an easy to use API for creating and managing multiple Python Interpreters. (\url{https://docs.python.org/3/library/multiprocessing.html})} to easily leverage parallel programming. While doing initial testing of the navigation and vision systems, it became clear that having a true state machine would have significant performance impacts, because Python is an interpreted language\footnote{A summary of the advantages and disadvantages of Interpreted Languages can be found here. (\url{https://en.wikipedia.org/wiki/Interpreted\_language})}, making it significantly slower than something ‘native’ or compiled like C/C++, which runs almost directly on the hardware. The team initially tried using the Python threading module\footnote{The threading module provided by Python is an easy to use API for creating and managing separate threads within a process. (\url{https://docs.python.org/3/library/threading.html})}, which enables a developer to have separate ‘threads’ or pieces of code running concurrently on the same Python interpreter. After further testing, it became apparent that threading would still be a performance bottleneck, as Python has something called the Global Interpreter Lock\footnote{Description of the Global Interpreter Lock. (\url{https://en.wikipedia.org/wiki/Global\_interpreter\_lock})} which locks code execution to one thread at a time. It was at this point that the team turned to the Python multiprocessing module, which allows all of the subsystems to be executed in parallel, providing the modularity and performance the team was looking for. Some of the functionality of the robot requires the separate processes to communicate\refapp{app:sysdiag}. Fortunately, the multiprocessing module has solutions for this, implementing easy to use wrappers for the standard Pipe and Shared Memory APIs provided by the Operating System\footnote{A short summary of different interprocess communication methods in Linux (\url{https://opensource.com/article/19/4/interprocess-communication-linux-storage})}.

To effectively coordinate the various subsystems, the team organized them into a class structure, with each class having a constructor, any helper methods it may need, and a ‘Run’ method which would run code in a separate process, if need be. This class structure shines when the team started thoroughly testing the platform’s subsystems, as all they had to do was import a module’s respective code, set up any extra imports or code, set up the multiprocessing, and run the systems in question. This made independently validating all of the software a simple endeavour and ensured it would be executed in precisely the same way no matter if it was a test script, or the actual robot bringup script.


\section{Results}
The nature of this robot requires many systems to work together in unison with a very tight tolerance to fully function. All systems and subsystems were able to be designed, fabricated, and mounted even with current events leading to some undesirable delays. With individual subsystems having differing success probabilities as described below, the overall system reached a start to finish final accuracy of slightly below 50\% in procurement of all possible points for the competition.

\subsection{Mechanical Droppers}
The mechanical droppers do deploy seeds into the seed receptacles when the robot is aligned correctly. In testing, the RobERT platform achieved an accuracy of around 50\% for the seed dispensing task due to an inability to perfectly align the robot to the degree required to be able to successfully actuate on all droppers. This was tested by allowing the robot to begin in the initial starting corner of the board and expanding itself and moving into position, autonomously deploying all droppers, and line following down the center of the competition board.

\subsection{Plant Detection}
The primary system issue was the reliability of plant position detection. Due to the decision to implement a closed-loop navigation system, the exact position of the robot in relation to the crops could not be known with confidence without external sensing. Effort was made to make a double use of the seed dispensing droppers as a tool for overall plant detection. When tested in the same way described above, the system would successfully detect the presence of 40-50 of each 64 plant run. Multiple possible solutions were in the process of being evaluated, yet due to many team members being unavailable during the mandatory work-from-home period, progress was not able to be swift enough to fully solve the issue prior to submission. As this or another method of localization is required for the system to properly trigger the image capture system, the RobERT platform was unable to consistently capture all required images to perform classification on.

\subsection{Plant Health Classification}
RobERT's vision classification system demonstrated vast success. Testing this system individually by means of positioning the robot in front of a plant and capturing images for each of the 64 positions on the board with randomized samples generated from the provided tool, the system experienced at worst a 60/64 accuracy of classifying between stressed, unstressed, and non-germinated positions, and at best perfect accuracy. Even in situations with difficult occlusion, the neural network would regularly still have upwards of 80\% confidence for the correct classification.

\subsection{Next Steps}
Should world events have happened differently to allow for more efficient work time with the team, other systems would have been added to function in parallel with current systems to increase reliability. While the elegance of the single solution for seed dispensing and plant detection was appreciated, the design was to be split to only focus on the actuation task for the mechanical droppers, and RGBD cameras would be added in conjunction with the other classification cameras to be able to construct a point cloud of the surroundings using a SLAM algorithm\footnote{SLAM: Simultaneous Localization and Mapping (\url{https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping})} to be able to map the surroundings to obtain an absolute position as well as being able to identify the best position for plant images to be captured. By making this change, the plant health determination task would be able to have the full success rate of the neural network, and the dropper design could be refined to optimize only for dispensing of seeds increasing the seed drop reliability.

\setcounter{secnumdepth}{3}
\section{Appendix}
\renewcommand{\thesubsection}{\Alph{subsection}}
\renewcommand{\thesubsubsection}{\Alph{subsection}.\arabic{subsubsection}}

\subsection{Mechanical} \label{app:mech}
\subsubsection{Parts List} \label{app:parts} 
    \begin{center}
    \nohyphens{
		\begin{tabular}{| p{3cm} | c | c | p{3cm} | p{5cm} |}
		\hline
			Part & Quantity & Price & Material & Purpose \\ \hline 
			Dynamixel AX-12 & 12 & Sponsor & Servo Motors & Actuating motors used for drive system and sensing for droppers \\ \hline	
			ArbotiX-M Robocontroller & 3 & \$119.85 & Electronic Component & Provides power distribution for Dynamixel motors, and one is used as a microcontroller for servo control \\ \hline
			Acrylic Sheets & 9 & \$270.00 & Acrylic & Laser cut acrylic used for lightweight expanding chassis design \\ \hline 
			Andymark 2” Stealth Wheels & 4 & \$28.00 & Rubber/Plastic & Provide necessary traction while navigating the course \\ \hline 
			Motor Mounts & 4 & N/A & 3D Printed PLA & Allow necessary mount points for {360\textdegree} steering \\ \hline 
			Drawer Slides & 4 & \$25.47 & Aluminum & Enable the expanding mechanism to expand linearly without sacrificing structural integrity or introducing excessive bend \\ \hline 
			Logitech Webcam Assembly & 1 & \$38.51 & Electronic Component & Downward facing camera used for line following and course navigation \\ \hline 
			Arducam & 4 & \$139.96 & Electronic Component & Used to scan rows of corn and record data \\ \hline 
			IGUS Cable Organizers & 2 & Sponsor & Plastic & Used to secure and hold wires while expanding and navigating the course \\ \hline 
			Camera Mounts  & 5 & N/A & 3D Printed PLA & Used to secure cameras in place \\ \hline 
			Smrza Micro Servo Motors & 4 & \$17.99 & Servo Motors & Used in expanding process to position cameras correctly \\ \hline 
			Google Coral Dev Board & 1 & \$129.99 & Electronic Component & Acts similarly to a Raspberry Pi but with tensor processing units to accelerate machine learning applications \\
		\hline	
		\end{tabular}
    }
    \end{center}

\subsubsection{Full Robot} \label{app:robot}
	\begin{center}
	    \frame{\includegraphics[scale=.15]{robot.jpg}}
	\end{center}

\subsubsection{Dropper Open Position} \label{app:dropopen}
	\begin{center}
	    \includegraphics[scale=.2]{dropopen.png}
	\end{center}

\subsubsection{Dropper Closed Position} \label{app:dropclose}
	\begin{center}
	    \includegraphics[scale=.2]{dropclose.png}
	\end{center}
	
\subsubsection{Phase I Panel Design} \label{app:phaseone}
	\begin{center}
	    \includegraphics[scale=.4]{Originaldesign.png}
	\end{center}
	
\subsubsection{Wheel Assembly for {360\textdegree} Steering} \label{app:WheelAssembly}
	\begin{center}
	    \includegraphics[scale=.4]{WheelAssembly.png}
	\end{center}
	
\subsubsection{Phase II Expanding Design} \label{app:phasetwo}
	\begin{center}
	    \includegraphics[scale=.4]{FullDesign.png}
	\end{center}

\subsubsection{Final Dropper Design} \label{app:Dropper}
	\begin{center}
	    \includegraphics[scale=.4]{Dropper.png}
	\end{center}
	
\subsubsection{Circuit Diagram} \label{app:circuit}
    \begin{center}
	    \includegraphics[scale=.45]{circuit.png}
	\end{center}

\subsection{General Software} \label{app:software}
\subsubsection{GitHub Repository} \label{app:github}
	\begin{center}
	    \url{https://github.com/Jyumpp/ASABE-2020}
	\end{center}

\subsection{Navigation and Drive System} \label{app:nav}
\subsubsection{Initial Line Following} \label{app:ogline}
    \begin{center}
        \frame{\includegraphics[scale=.4]{ogline.png}}
    \end{center}

\subsection{Vision System} \label{app:vision}
\subsubsection{Uncropped Image} \label{app:uncrop}
	\begin{center}
	    \frame{\includegraphics[scale=.2]{uncropped.jpg}}
	\end{center}

\subsubsection{Cropped Image} \label{app:crop}
	\begin{center}
	    \frame{\includegraphics[scale=.2]{cropped.jpg}}
	\end{center}
	
\subsubsection{Video of Original Trigger System} \label{app:ogtrig}
	\begin{center}
	    \url{https://youtu.be/Y0aVu4daRNE}
	\end{center}

\subsubsection{Dynamixel Trigger System} \label{app:dyntrig}
	\begin{center}
	    \url{https://youtu.be/hV4fYE3qgEY}
	\end{center}


\subsubsection{Initial Image Classification} \label{app:ogimg}
	\begin{center}
	    \frame{
	    \includegraphics[scale=0.4]{plant square.jpg}
	    \includegraphics[scale=0.4]{color_percents_graph.jpg}}
	\end{center}

\subsubsection{Screenshot of Utility} \label{app:util}
	\begin{center}
	    \frame{\includegraphics[scale=0.5]{image_cap_utility.png}}
	\end{center}

\subsubsection{Model Training} \label{app:model}
	\begin{center}
	     \includegraphics[scale=0.45]{training_epochs.png}
	     \includegraphics[scale=0.9]{chart.png}
	\end{center}


\subsection{Software Systems Integration} \label{app:system}
\subsubsection{Software Systems Flow Diagram} \label{app:sysdiag}
	\begin{center}
	    \includegraphics[scale=0.60]{softwaresystemsflow.png}
	\end{center}
Solid-outlined objects are running in separate processes, dotted-outlined objects are running in the same process as the parent, but on a separate thread.
\end{document}